quran <- Corpus(DirSource("/Users/sabreenaabedin/Desktop/text-mining-bootcamp/files/3-Quran-Islam.txt"),readerControl = list(language="eng"))
library(tm)
library(wordcloud)
library(SnowballC)
library(ggplot2)
quran <- Corpus(DirSource("/Users/sabreenaabedin/Desktop/text-mining-bootcamp/files/3-Quran-Islam.txt"),readerControl = list(language="eng"))
dox <- Corpus("/Users/sabreenaabedin/Desktop/text-mining-bootcamp/files",readerControl = list(language="eng"))
dox <- Corpus("/Users/sabreenaabedin/Desktop/text-mining-bootcamp/files/3-Quran-Islam.txt")
quran <- Corpus(DirSource(paste(localpath, "/quran", sep="")),readerControl = list(language="eng"))
localpath <- "/Users/sabreenaabedin/Desktop/text-mining-bootcamp/files"
library(tm)
library(wordcloud)
library(SnowballC)
library(ggplot2)
D
dox <- Corpus(DirSource(localpath),readerControl = list(language="eng"))
bible <- Corpus(DirSource(paste(localpath, "/bible", sep="")),readerControl = list(language="eng"))
bible <- Corpus(DirSource(paste(localpath, "/bible", sep="")),readerControl = list(language="eng"))
mormon <- Corpus(DirSource(paste(localpath, "/mormon", sep="")),readerControl = list(language="eng"))
localpath <- "/Users/sabreenaabedin/Desktop/text-mining-bootcamp"
library(tm)
library(wordcloud)
library(SnowballC)
library(ggplot2)
bible <- Corpus(DirSource(paste(localpath, "/bible", sep="")),readerControl = list(language="eng"))
bible <- tm_map(bible,content_transformer(tolower))
bible <- tm_map(bible, PlainTextDocument)
bible <- tm_map(bible,stripWhitespace)
bible <- tm_map(bible,removeWords,stopwords('english'))
bible <- tm_map(bible,removePunctuation)
bible <- tm_map(bible,stemDocument)
bible <- tm_map(bible, removeNumbers)
dtm <- DocumentTermMatrix(bible)
tdm <- TermDocumentMatrix(bible)
inspect(dtm[1:3, 1:5])
dtm <- DocumentTermMatrix(bible)
tdm <- TermDocumentMatrix(bible)
findAssocs(dtm, "god", corlimit=0.01)
findAssocs(dtm, "god", 0.99)
wordcloud(bible,scale=c(5,0.5),max.words=80, random.order = FALSE, rot.per = .25, colors = RColorBrewer::brewer.pal(8,"Dark2"))
inspect(dtm[1:3,1:3]) #verify that sum of the rows would give total number of words
rowTotals <- apply(dtm, 1, sum)
View(rowTotals)
barplot(rowTotals, main="Terms per Document", xlab = "Word Count",
ylab="Document", col="darksalmon", horiz = TRUE,
names.arg=c("Mormon", "Bible", "Quran", "Buddh.", "Zend.", "Med."))
findFreqTerms(dtm,100)
freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing=FALSE) # terms in ascending order
fterms <- freq[tail(ord, 20)] # grab last 20 terms
tail(fterms) # verify
my.df <- as.data.frame(fterms)
my.df.scale <- scale(my.df) #normalize
d <- dist(my.df.scale,method="euclidean") #find euclidean distance
fit <- hclust(d, method="ward.D")
plot(fit, col = "indianred4", xlab = "Terms") #plot
m <- as.matrix(dtm)
d <- dist(m)
groups <- hclust(d, method="ward.D")
plot(groups, hang = -1)
rect.hclust(groups,2)
rect.hclust(groups, 4) # k = 4
positives= readLines(paste(localpath, "/positive_words.txt", sep=""))
negatives= readLinespaste(localpath, "/negative_words.txt")
negatives= readLinespaste(localpath, "/negative_words.txt"))
negatives= readLines(paste(localpath, "/negative_words.txt"))
negatives= readLines(paste(localpath, "/negative_words.txt", sept=""))
positives= readLines(paste(localpath, "/positive_words.txt", sep=""))
negatives= readLines(paste(localpath, "/negative_words.txt", sept=""))
negatives= readLines(paste(localpath, "/negative_words.txt", sept=""))
negatives= readLines("/Users/sabreenaabedin/Desktop/text-mining-bootcamp/negative_words.txt")
negatives= readLines(paste(localpath, "/negative_words.txt", sep=""))
rm(negatives)
negatives= readLines(paste(localpath, "/negative_words.txt", sep=""))
score.sentiment = function(sentences, pos.words, neg.words)
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, pos.words, neg.words) {
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = tolower(sentence)
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
SentimentScores <- as.data.frame(c("Mormon", "Bible", "Quran", "Buddh.", "Zend.", "Med."))
SentimentScores[c("scores")] <- NA
View(SentimentScores)
Bible <- readLines(paste(localpath, "/bible/2-King-James-Bible-Christianity.txt", sep=""))
Score <- score.sentiment(Bible,positives,negatives)
hist(Score$score,xlab="Sentiment Score ",main="Bible Sentiment",
border="black",col="darkseagreen")
SentimentScores[2,2] <- sum(Score$score)
View(SentimentScores)
SentimentScores[1,2] <- SentimentScores[1,2]/126447
findFrequency = function(text)
{
## attempted iterative approach at first
# violentfrequency <- 0
# for(i in 1:nrow(data.frame)){
# n <- length(grep(text, data.frame[i]))
# violentfrequency <- violentfrequency + n
# }
## ended up hard coding the violent words
violentfrequency <- 0
violentfrequency <- length(grep("wound", text)) + length(grep("hurt", text)) +
length(grep("fight", text)) + length(grep("violate", text)) + length(grep("destroy", text)) +
length(grep("slaughter", text)) + length(grep("murder", text)) + length(grep("kill", text))
+ length(grep("attack", text)) + length(grep("break", text)) + length(grep("crush", text))
+ length(grep("provoke", text)) + length(grep("anger", text)) + length(grep("hatred", text))
return(violentfrequency)
}
violence <- as.data.frame(c("Mormon", "Bible", "Quran", "Buddh.", "Zend.", "Med."))
violence[c("scores")] <- NA
violence[2,2] <- findFrequency(Bible)
View(violence)
plot(violence, xlab = "text")
violence[2,2] <- violence[2,2]/373701
plot(violence, xlab = "text")
localpath <- "/Users/sabreenaabedin/Desktop/text-mining-bootcamp"
library(tm)
library(wordcloud)
library(SnowballC)
library(ggplot2)
dox <- Corpus(DirSource(paste(localpath, "/files", sep="")),readerControl = list(language="eng"))
quran <- Corpus(DirSource(paste(localpath, "/quran", sep="")),readerControl = list(language="eng"))
bible <- Corpus(DirSource(paste(localpath, "/bible", sep="")),readerControl = list(language="eng"))
mormon <- Corpus(DirSource(paste(localpath, "/mormon", sep="")),readerControl = list(language="eng"))
buddha <- Corpus(DirSource(paste(localpath, "/buddha", sep="")),readerControl = list(language="eng"))
zoroastrian <- Corpus(DirSource(paste(localpath, "/zoroastrian", sep="")),readerControl = list(language="eng"))
meditation <- Corpus(DirSource(paste(localpath, "/meditation", sep="")),readerControl = list(language="eng"))
quran <- tm_map(quran,content_transformer(tolower))
quran <- tm_map(quran,stripWhitespace)
quran <- tm_map(quran,removeWords,stopwords('english'))
quran <- tm_map(quran,removePunctuation)
quran <- tm_map(quran,stemDocument)
quran <- tm_map(quran, removeNumbers)
quran <- tm_map(quran, PlainTextDocument)
bible <- tm_map(bible,content_transformer(tolower))
bible <- tm_map(bible, PlainTextDocument)
bible <- tm_map(bible,stripWhitespace)
bible <- tm_map(bible,removeWords,stopwords('english'))
bible <- tm_map(bible,removePunctuation)
bible <- tm_map(bible,stemDocument)
bible <- tm_map(bible, removeNumbers)
mormon <- tm_map(mormon,content_transformer(tolower))
mormon <- tm_map(mormon, PlainTextDocument)
mormon <- tm_map(mormon,stripWhitespace)
mormon <- tm_map(mormon,removeWords,stopwords('english'))
mormon <- tm_map(mormon,removePunctuation)
mormon <- tm_map(mormon,stemDocument)
mormon <- tm_map(mormon, removeNumbers)
buddha <- tm_map(buddha,content_transformer(tolower))
buddha <- tm_map(buddha, PlainTextDocument)
buddha <- tm_map(buddha,stripWhitespace)
buddha <- tm_map(buddha,removeWords,stopwords('english'))
buddha <- tm_map(buddha,removePunctuation)
buddha <- tm_map(buddha,stemDocument)
buddha <- tm_map(buddha, removeNumbers)
meditation <- tm_map(meditation,content_transformer(tolower))
meditation <- tm_map(meditation, PlainTextDocument)
meditation <- tm_map(meditation,stripWhitespace)
meditation <- tm_map(meditation,removeWords,stopwords('english'))
meditation <- tm_map(meditation,removePunctuation)
meditation <- tm_map(meditation,stemDocument)
meditation <- tm_map(meditation, removeNumbers)
dtm <- DocumentTermMatrix(bible)
tdm <- TermDocumentMatrix(bible)
inspect(dtm[1, 1:5])
tdm.common <- removeSparseTerms(tdm, .1)
dtm.common <-removeSparseTerms(dtm, 0.6)
inspect(dtm.common)
findAssocs(dtm, "god", corlimit=0.01)
findAssocs(dtm, "god", 0.99)
wordcloud(dox,scale=c(5,0.5),max.words=80, random.order = FALSE, rot.per = .25, colors = RColorBrewer::brewer.pal(8,"Dark2"))
wordcloud(quran,scale=c(5,0.5),max.words=80, random.order = FALSE, rot.per = .25, colors = RColorBrewer::brewer.pal(8,"Dark2"))
inspect(dtm[1:3,1:3]) #verify that sum of the rows would give total number of words
inspect(dtm[1,1:3]) #verify that sum of the rows would give total number of words
rowTotals <- apply(dtm, 1, sum)
View(rowTotals)
barplot(rowTotals, main="Terms per Document", xlab = "Word Count",
ylab="Document", col="darksalmon", horiz = TRUE,
names.arg=c("Mormon", "Bible", "Quran", "Buddh.", "Zend.", "Med."))
findFreqTerms(dtm,100)
freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing=FALSE) # terms in ascending order
fterms <- freq[tail(ord, 20)] # grab last 20 terms
tail(fterms) # verify
my.df <- as.data.frame(fterms)
my.df.scale <- scale(my.df) #normalize
d <- dist(my.df.scale,method="euclidean") #find euclidean distance
fit <- hclust(d, method="ward.D")
plot(fit, col = "indianred4", xlab = "Terms") #plot
m <- as.matrix(dtm)
d <- dist(m)
groups <- hclust(d, method="ward.D")
plot(groups, hang = -1)
rect.hclust(groups,2)
rect.hclust(groups, 4) # k = 4
dtmq <- DocumentTermMatrix(quran)
m <- as.matrix(dtmq)
d <- dist(m)
groups <- hclust(d, method="ward.D")
plot(groups, hang = -1)
findFreqTerms(dtm,100)
Mormon <- readLines(paste(localpath, "/1-Book-of-Mormon-Mormonism.txt", sep=""))
Mormon <- readLines(paste(localpath, "/files/1-Book-of-Mormon-Mormonism.txt", sep=""))
Score <- score.sentiment(Mormon,positives,negatives)
positives= readLines(paste(localpath, "/positive_words.txt", sep=""))
negatives= readLines(paste(localpath, "/negative_words.txt", sep=""))
score.sentiment = function(sentences, pos.words, neg.words)
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, pos.words, neg.words) {
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = tolower(sentence)
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
SentimentScores <- as.data.frame(c("Mormon", "Bible", "Quran", "Buddh.", "Zend.", "Med."))
SentimentScores[c("scores")] <- NA
View(SentimentScores)
Mormon <- readLines(paste(localpath, "/files/1-Book-of-Mormon-Mormonism.txt", sep=""))
Score <- score.sentiment(Mormon,positives,negatives)
hist(Score$score,xlab="Sentiment Score ",main="Mormon Sentiment",
border="black",col="darkseagreen")
SentimentScores[1,2] <- sum(Score$score)
Bible <- readLines(paste(localpath, "/files/2-King-James-Bible-Christianity.txt", sep=""))
Score <- score.sentiment(Bible,positives,negatives)
hist(Score$score,xlab="Sentiment Score ",main="Bible Sentiment",
border="black",col="darkseagreen")
SentimentScores[2,2] <- sum(Score$score)
Quran <- readLines(paste(localpath, "/files/3-Quran-Islam.txt", sep=""))
Score <- score.sentiment(Quran,positives,negatives)
hist(Score$score,xlab="Sentiment Score ",main="Quran Sentiment",
border="black",col="darkseagreen")
dtm <- DocumentTermMatrix(bible)
tdm <- TermDocumentMatrix(bible)
inspect(dtm[1, 1:5])
inspect(tdm[1:5, 1])
inspect(dtm.common)
findAssocs(dtm, "god", corlimit=0.01)
findAssocs(dtm, "god", 0.99)
findAssocs(dtm, "god", 0.99)
findAssocs(dtm, "god", corlimit=0.01)
findAssocs(dtm, "god", corlimit=0.99)
findAssocs(dtm, "god", corlimit=0.7)
findFreqTerms(dtm,100)
findFreqTerms(dtm,1000)
wordcloud(dox,scale=c(5,0.5),max.words=80, random.order = FALSE, rot.per = .25, colors = RColorBrewer::brewer.pal(8,"Dark2"))
wordcloud(quran,scale=c(5,0.5),max.words=80, random.order = FALSE, rot.per = .25, colors = RColorBrewer::brewer.pal(8,"Dark2"))
wordcloud(bible,scale=c(5,0.5),max.words=80, random.order = FALSE, rot.per = .25, colors = RColorBrewer::brewer.pal(8,"Dark2"))
wordcloud(mormon,scale=c(5,0.5),max.words=80, random.order = FALSE, rot.per = .25, colors = RColorBrewer::brewer.pal(8,"Dark2"))
inspect(dtm[1,1:3]) #verify that sum of the rows would give total number of words
rowTotals <- apply(dtm, 1, sum)
View(rowTotals)
barplot(rowTotals, main="Terms per Document", xlab = "Word Count",
ylab="Document", col="darksalmon", horiz = TRUE,
names.arg=c("Mormon", "Bible", "Quran", "Buddh.", "Zend.", "Med."))
findFreqTerms(dtm,100)
freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing=FALSE) # terms in ascending order
fterms <- freq[tail(ord, 20)] # grab last 20 terms
tail(fterms) # verify
my.df <- as.data.frame(fterms)
my.df.scale <- scale(my.df) #normalize
d <- dist(my.df.scale,method="euclidean") #find euclidean distance
fit <- hclust(d, method="ward.D")
plot(fit, col = "indianred4", xlab = "Terms") #plot
localpath <- "/Users/sabreenaabedin/Desktop/text-mining-bootcamp/sabreena-demo"
dox <- Corpus(DirSource(paste(localpath, "/files", sep="")),readerControl = list(language="eng"))
library(tm)
dox <- Corpus(DirSource(paste(localpath, "/files", sep="")),readerControl = list(language="eng"))
setwd("/Users/sabreenaabedin/Desktop/class/SYS4021") #SABREENA'S
source("AccidentInput.R")
source("SPM_Panel.R")
source("TestSet.R")
library(lattice)
path <- "/Users/sabreenaabedin/Desktop/class/SYS4021/TrainData" #SABREENA'S
acts <- file.inputl(path)
comvar <- intersect(colnames(acts[[1]]), colnames(acts[[8]]))
totacts <- combine.data(acts, comvar)
rm(comvar)
totacts$Casualty <- totacts$TOTINJ + totacts$TOTKLD
totacts <- totacts[!duplicated(totacts[,c("INCDTNO", "YEAR", "MONTH", "DAY", "TIMEHR", "TIMEMIN")]),]
totacts$Cause <- rep(NA, nrow(totacts))
totacts$Cause[which(substr(totacts$CAUSE, 1, 1) == "M")] <- "M"
totacts$Cause[which(substr(totacts$CAUSE, 1, 1) == "T")] <- "T"
totacts$Cause[which(substr(totacts$CAUSE, 1, 1) == "S")] <- "S"
totacts$Cause[which(substr(totacts$CAUSE, 1, 1) == "H")] <- "H"
totacts$Cause[which(substr(totacts$CAUSE, 1, 1) == "E")] <- "E"
totacts$Cause <- factor(totacts$Cause)
totacts$Type <- factor(totacts$TYPE, labels = c("Derailment", "HeadOn", "Rearend", "Side", "Raking", "BrokenTrain", "Hwy-Rail", "GradeX", "Obstruction", "Explosive", "Fire","Other","SeeNarrative"))
xcasnd <- totacts[totacts$Casualty >= 1,]
response.pca <- princomp(xcasnd[,c("TOTINJ", "TOTKLD", "TRKDMG", "ACCDMG", "EQPDMG")], cor=T)
response.pca <- princomp(totacts[,c("TOTINJ", "TOTKLD", "TRKDMG", "ACCDMG", "EQPDMG")], cor=T)
biplot(response.pca)
response.pca.cas <- princomp(xcasndt14[,c("TOTINJ", "TOTKLD", "TRKDMG", "ACCDMG", "EQPDMG")], cor=T)
response.pca <- princomp(totacts[,c("Casualty", "TOTINJ", "TOTKLD", "TRKDMG", "ACCDMG", "EQPDMG")], cor=T)
biplot(response.pca)
summary(response.pca)
loadings(response.pca)
summary(lm.acc4.step)$adj.r.squared
lm.acc4<-lm(ACCDMG~(HIGHSPD+CARS +Cause+Type)^2+I(TEMP^2)+I(HIGHSPD^2)+I(CARS^2),data=xactsndt25)
lm.acc4.step <- step(lm.acc4, direction = "both")
lm.acc4<-lm(ACCDMG~(HIGHSPD+CARS +Cause+Type)^2+I(TEMP^2)+I(HIGHSPD^2)+I(CARS^2),data=xactsndt25)
